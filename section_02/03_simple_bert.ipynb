{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snufkin92/nlp_bert/blob/master/section_02/03_simple_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv454lBK1YCc"
      },
      "source": [
        "# シンプルなBERTの実装\n",
        "訓練済みのモデルを使用し、文章の一部の予測、及び2つの文章が連続しているかどうかの判定を行います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_Ozfz3NhltP"
      },
      "source": [
        "## ライブラリのインストール\n",
        "PyTorch-Transformers、および必要なライブラリのインストールを行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_mDYVlb-sqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6d33d4-c61f-42f0-a58f-74c60ad02bad"
      },
      "source": [
        "!pip install folium==0.2.1\n",
        "!pip install urllib3==1.25.11\n",
        "!pip install pytorch-transformers==1.2.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from folium==0.2.1) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->folium==0.2.1) (2.1.5)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79793 sha256=a9cf191756f432ee70feb5621932d8a34e5e7823921de1727881c2e91943f1e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/0c/07/d7792a5444d5bb074361ac27da53cee9d5cce59a07fe9da5dd\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.17.0\n",
            "    Uninstalling folium-0.17.0:\n",
            "      Successfully uninstalled folium-0.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.17.6 requires folium>=0.9.1, but you have folium 0.2.1 which is incompatible.\n",
            "geemap 0.33.1 requires folium>=0.13.0, but you have folium 0.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed folium-0.2.1\n",
            "Collecting urllib3==1.25.11\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "Successfully installed urllib3-1.25.11\n",
            "Collecting pytorch-transformers==1.2.0\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (1.26.4)\n",
            "Collecting boto3 (from pytorch-transformers==1.2.0)\n",
            "  Downloading boto3-1.34.153-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (4.66.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (2024.5.15)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (0.1.99)\n",
            "Collecting sacremoses (from pytorch-transformers==1.2.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting botocore<1.35.0,>=1.34.153 (from boto3->pytorch-transformers==1.2.0)\n",
            "  Downloading botocore-1.34.153-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-transformers==1.2.0)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch-transformers==1.2.0)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers==1.2.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers==1.2.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers==1.2.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers==1.2.0) (2024.7.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.153->boto3->pytorch-transformers==1.2.0) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch-transformers==1.2.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch-transformers==1.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.153->boto3->pytorch-transformers==1.2.0) (1.16.0)\n",
            "Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading boto3-1.34.153-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m232.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.34.153-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.34.153 botocore-1.34.153 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pytorch-transformers-1.2.0 s3transfer-0.10.2 sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb5gVyYF2vjL"
      },
      "source": [
        "## 文章の一部の予測\n",
        "文章における一部の単語をMASKし、それをBERTのモデルを使って予測します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6oFZnS21Mqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e893f810-e045-4254-9bbc-3dad01b20fe9"
      },
      "source": [
        "import torch\n",
        "from pytorch_transformers import BertForMaskedLM\n",
        "from pytorch_transformers import BertTokenizer\n",
        "\n",
        "# 「CLS]、「SEP] はBERTにとって特別な意味(文章の開始と終了）を持つ\n",
        "text = \"[CLS] I played baseball with my friends at school yesterday [SEP]\"\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "words = tokenizer.tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1292645.92B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'i', 'played', 'baseball', 'with', 'my', 'friends', 'at', 'school', 'yesterday', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Y32Tl55dSl"
      },
      "source": [
        "文章の一部をMASKします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_H50V7b5RM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10de4ff6-5aea-4323-a4c0-da96fc5365ca"
      },
      "source": [
        "msk_idx = 3\n",
        "\n",
        "# 3番目のインデックス（baseball）をマスク\n",
        "words[msk_idx] = \"[MASK]\"  # 単語を[MASK]に置き換える\n",
        "print(words)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'i', 'played', '[MASK]', 'with', 'my', 'friends', 'at', 'school', 'yesterday', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBXgAn9s528_"
      },
      "source": [
        "単語を対応するインデックスに変換します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm4qbMPW56-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf29e40c-108a-47ba-c045-d5a477d9d3e5"
      },
      "source": [
        "# 事前学習済のBERT単語辞書の対応するインデックスに変換\n",
        "# もし単語辞書に存在しない単語 (Out-of-Vocabulary, OOV) があった場合、\n",
        "# BERT トークナイザーはそれをサブワードに分割するか、特別なトークン [UNK] (Unknown) に置き換える。\n",
        "# 例：unbelievableが登録されていなかった場合、\n",
        "## \"un#と \"#believ\" と \"##able\" に分割する可能性がある\n",
        "# 　\"believ\" と \"able\" は単語辞書の中に存在し、単語の途中に現れることを示すために、\n",
        "# 接頭語として \"##\" が付けられる。\n",
        "# ## は BERT ではトークンの位置情報を示す特別な記号としての意味を持つ\n",
        "# ただし \"believ\"は単語辞書に存在しないため \"believ\"のインデックスに\n",
        "# サブワードである事を示す情報が付加される（Position Embeddingで解決）\n",
        "word_ids = tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "print(\"### type(word_ids) = \", type(word_ids))\n",
        "print(word_ids)\n",
        "\n",
        "word_tensor = torch.tensor([word_ids])  # テンソルに変換\n",
        "print(\"### type(word_tensor) = \", type(word_tensor))\n",
        "print(word_tensor)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### type(word_ids) =  <class 'list'>\n",
            "[101, 1045, 2209, 103, 2007, 2026, 2814, 2012, 2082, 7483, 102]\n",
            "### type(word_tensor) =  <class 'torch.Tensor'>\n",
            "tensor([[ 101, 1045, 2209,  103, 2007, 2026, 2814, 2012, 2082, 7483,  102]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNHei2g3Yk2a",
        "outputId": "a7b60ff9-b785-4794-f3dc-24d2903f0200"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y76O88877cB_"
      },
      "source": [
        "BERTのモデルを使って予測を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2NWREc77gQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee6dc792-d4e4-47dd-c5ff-56fa99b2b502"
      },
      "source": [
        "msk_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# GPUだと数秒、CPUだと絶望的に遅い\n",
        "msk_model.cuda()  # GPU対応\n",
        "msk_model.eval() # 学習は行わず予測モード\n",
        "\n",
        "x = word_tensor.cuda()  # GPU対応\n",
        "# x = word_tensor # CPU用のテンソル\n",
        "y = msk_model(x)  # 予測\n",
        "print(\"### type(y) = \", type(y))\n",
        "result = y[0]\n",
        "\n",
        "# len(word_ids) = 11\n",
        "# torch.Size([1, 11, 30522]) = [バッチサイズ、単語数、単語数]\n",
        "print(result.size())  # 結果の形状\n",
        "\n",
        "# \"_\"はTop5の単語のスコア（対数尤度）\n",
        "_, top5_indices = torch.topk(result[0][msk_idx], k=5)  # 最も大きい5つの値\n",
        "print(\"### _ = \", _)\n",
        "print(\"### max_ids = \", max_ids)\n",
        "\n",
        "# [MASK]に入る部分の単語\n",
        "result_words = tokenizer.convert_ids_to_tokens(top5_indices.tolist())  # インデックスを単語に変換\n",
        "print(result_words)\n",
        "\n",
        "# result_wordsに対する確率を出力\n",
        "prob = torch.nn.functional.softmax(result[0][msk_idx][top5_indices], dim=0)\n",
        "print(prob)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### type(y) =  <class 'tuple'>\n",
            "torch.Size([1, 11, 30522])\n",
            "### _ =  tensor([10.5885, 10.5762,  9.9564,  9.8097,  9.1332], device='cuda:0',\n",
            "       grad_fn=<TopkBackward0>)\n",
            "### max_ids =  tensor([3455, 2374, 4715, 3598, 5093], device='cuda:0')\n",
            "['basketball', 'football', 'soccer', 'baseball', 'tennis']\n",
            "tensor([0.3114, 0.3076, 0.1655, 0.1429, 0.0727], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### result[0] = \", result[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg1zBT3Ua5Om",
        "outputId": "72083f28-f179-4731-e1bd-ea284303986f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### result[0] =  tensor([[ -6.6873,  -6.6405,  -6.6409,  ...,  -6.0201,  -5.8183,  -3.9777],\n",
            "        [ -9.5150,  -9.3415,  -9.3818,  ...,  -8.4236,  -8.4428,  -5.3152],\n",
            "        [-10.0568, -10.1768, -10.2753,  ...,  -8.5044,  -8.6216,  -5.3011],\n",
            "        ...,\n",
            "        [-13.6662, -14.2769, -13.8572,  ..., -12.8681, -11.8016, -11.4662],\n",
            "        [ -9.2015,  -8.9383,  -9.3056,  ...,  -7.7869,  -9.2608,  -3.0500],\n",
            "        [-13.1242, -12.9603, -12.7900,  ...,  -9.9769, -10.1773, -10.8939]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 勿論、out of bounds\n",
        "print(\"### result[1] = \", result[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "u18mh4EOa8JE",
        "outputId": "419098ce-3623-425e-ea8a-dfb4fcd8815a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 1 is out of bounds for dimension 0 with size 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-23f0c17f56be>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"### result[1] = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 入力各単語の確率（30522語の中での確率）\n",
        "print(\"### result[0].shpae] = \", result[0].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rgg5B0mbCZy",
        "outputId": "d096f25e-3379-43d2-ae25-d459ec0cdca5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### result[0].shpae] =  torch.Size([11, 30522])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Su6QTCAAgFS"
      },
      "source": [
        "## 文章が連続しているかどうかの判定\n",
        "BERTのモデルを使って、2つの文章が連続しているかどうかの判定を行います。  \n",
        "以下の関数`show_continuity`では、2つの文章の連続性を判定し、表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC0nihWMAtgG"
      },
      "source": [
        "from pytorch_transformers import BertForNextSentencePrediction\n",
        "\n",
        "def show_continuity(text, seg_ids):\n",
        "    \"\"\"\n",
        "    text: 2つの文章を\"[CLS]\"と\"[SEP]\"トークンで区切って結合した文字列。\n",
        "        例: \"[CLS] 今日はいい天気です。[SEP] 公園でピクニックをします。[SEP]\"\n",
        "\n",
        "    seg_ids: 各トークンがどちらの文章に属するかを示す数値のリスト。\n",
        "             最初の文章のトークンには0、2つ目の文章のトークンには1が割り当てられる。\n",
        "        例: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
        "\n",
        "    \"\"\"\n",
        "    words = tokenizer.tokenize(text)\n",
        "    word_ids = tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "    word_tensor = torch.tensor([word_ids])  # テンソルに変換\n",
        "\n",
        "    seg_tensor = torch.tensor([seg_ids])\n",
        "\n",
        "    nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "    nsp_model.cuda()  # GPU対応\n",
        "    nsp_model.eval()\n",
        "\n",
        "    x = word_tensor.cuda()  # GPU対応\n",
        "    s = seg_tensor.cuda()  # GPU対応\n",
        "\n",
        "    y = nsp_model(x, s)  # 予測\n",
        "    result = torch.softmax(y[0], dim=1)\n",
        "    print(result)  # Softmaxで確率に\n",
        "    print(str(result[0][0].item()*100) + \"%の確率で連続しています。\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWogb8nFIQMg"
      },
      "source": [
        "`show_continuity`関数に、自然につながる2つの文章を与えます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaUmof1yF_rD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89e425e-c6eb-4ce5-94ab-d4e8b8b574e0"
      },
      "source": [
        "text = \"[CLS] What is baseball ? [SEP] It is a game of hitting the ball with the bat [SEP]\"\n",
        "seg_ids = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1, 1]  # 0:前の文章の単語、1:後の文章の単語\n",
        "show_continuity(text, seg_ids)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 4.5869e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "99.9995470046997%の確率で連続しています。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKjotaOCIeeK"
      },
      "source": [
        "`show_continuity`関数に、自然につながらない2つの文章を与えます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4qAKBlcGRYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57e3f3e-51b7-4fc7-dd01-62e622421195"
      },
      "source": [
        "text = \"[CLS] What is baseball ? [SEP] This food is made with flour and milk [SEP]\"\n",
        "seg_ids = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 0:前の文章の単語、1:後の文章の単語\n",
        "show_continuity(text, seg_ids)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[9.5296e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "0.0009529620001558214%の確率で連続しています。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "お遊び"
      ],
      "metadata": {
        "id": "KxkArXrljWW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"[CLS] I love you [SEP] Me too [SEP]\" # 99.99316930770874%\n",
        "# text = \"[CLS] I love you [SEP] Kick ass [SEP]\" # 7.2146937251091%\n",
        "text = \"[CLS] I love you [SEP] Fuck you [SEP]\"\n",
        "seg_ids = [0, 0, 0, 0, 0, 1, 1, 1]  # 0:前の文章の単語、1:後の文章の単語\n",
        "show_continuity(text, seg_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBriEPi9hBAI",
        "outputId": "cded26fe-782e-4636-9ade-f79fbf283285"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9984, 0.0016]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "99.84017014503479%の確率で連続しています。\n"
          ]
        }
      ]
    }
  ]
}